import jax
import jax.numpy as jnp
import optax
import finitediffx as fdx
import jaxopt

#----Introduction----
#This MRE is paired with jaxopt_mre_gemini.py.
#This MRE uses fixed data, optax, a placeholder for the diffeq solve, and a custom vjp implementing the 
#IFT through the inner optax optmizer

#----Purpose----
#The MRE compares the gradient estimated by finite differences over system described in the intro
#to the gradient estimated by AD over the same system where that AD contains a custom vjp defined below. 

#----Result----
#The loss generated by the two implementation is the same, but the gradients do not match

#----Conclusion----
#Running this example demonstrates that either:
#1) Something related to using optax as the inner optmizer leads to numerical instability
#   in the estimate of the gradient components leading to the custom vjp failing despite the 
#   mathematics in the vjp being correct. 
#   
#   OR
#
#2) The custom vjp mathematics are wrong. 

# ===================================================================
# 1. Hardcoded Inputs
# ===================================================================
# Parameters to be optimized (a flat vector)
opt_params = jnp.array([
    0.47, 1.09, 3.55,  # pop_coeff for ka, cl, vd
    -1.386,            # log(sigma^2) -> so sigma^2 = 0.25
    -0.51, 0.0, -1.20,  # omega cholesky factors
    0.0, 0.0, -2.30
], dtype=jnp.float64)

# Static data for a single subject
padded_y_i = jnp.array([0.74, 2.84, 6.57, 10.5, 9.66, 8.58, 8.36, 7.47, 6.89, 5.94, 3.28], dtype=jnp.float64)
time_mask_y_i = jnp.ones_like(padded_y_i, dtype=bool)
data_contrib_i = jnp.zeros(3, dtype=jnp.float64)
initial_b_i = jnp.zeros(3, dtype=jnp.float64)
pop_coeff_w_bi_idx = jnp.array([0, 1, 2])

# ===================================================================
# 2. Helper Functions (Unpacking, Inner Loss)
# ===================================================================
def unpack_params(params):
    """Unpacks the flat vector into structured parameters."""
    pop_coeff = params[0:3]
    sigma2 = jnp.exp(params[3]) # Note: No squaring, exp(log(sigma^2))
    omega_chol_vals = params[4:10]
    
    omega_lchol = jnp.zeros((3, 3), dtype=jnp.float64).at[jnp.tril_indices(3)].set(omega_chol_vals)
    omega_lchol = omega_lchol.at[jnp.diag_indices(3)].set(jnp.exp(jnp.diag(omega_lchol)))
    omega2 = omega_lchol @ omega_lchol.T
    
    return pop_coeff, jnp.array([sigma2]), omega2 # Ensure sigma2 is shape (1,)

def toy_inner_loss(b_i, pop_coeff, sigma2, omega2):
    """Simplified inner loss using a linear model."""
    model_coeffs_i = jnp.exp(data_contrib_i + pop_coeff + b_i)
    
    A = jnp.eye(time_mask_y_i.shape[0], model_coeffs_i.shape[0])
    pred_y_i = A @ model_coeffs_i
    
    residuals_i = jnp.where(time_mask_y_i, padded_y_i - pred_y_i, 0.0)
    sum_sq_residuals = jnp.sum(residuals_i**2)
    
    loss_data = jnp.sum(time_mask_y_i) * jnp.log(sigma2[0]) + sum_sq_residuals / sigma2[0]
    
    L, _ = jax.scipy.linalg.cho_factor(omega2, lower=True)
    log_det_omega2 = 2 * jnp.sum(jnp.log(jnp.diag(L)))
    prior_penalty = b_i @ jax.scipy.linalg.cho_solve((L, True), b_i)
    
    return loss_data + log_det_omega2 + prior_penalty

# ===================================================================
# 3. The Inner b_i Estimator using optax and custom vjp
# ===================================================================
def _estimate_b_i_impl(pop_coeff, sigma2, omega2):
    """Forward pass: finds b_i and calculates values needed for VJP."""
    obj_fn = lambda b_i: toy_inner_loss(b_i, pop_coeff, sigma2, omega2)
    
    use_optax = False
    if use_optax:
        optimizer = optax.adam(0.05)
        opt_state = optimizer.init(initial_b_i)
        grad_fn = jax.grad(obj_fn)

        def update_step(i, state):
            params, opt_state = state
            grads = grad_fn(params)
            updates, opt_state = optimizer.update(grads, opt_state, params)
            return optax.apply_updates(params, updates), opt_state

        estimated_b_i, _ = jax.lax.fori_loop(0, 10000, update_step, (initial_b_i, opt_state))
    else:
        solver = jaxopt.LBFGS(fun=obj_fn, tol=1e-5, maxiter=500)
        solution = solver.run(initial_b_i)
        estimated_b_i = solution.params
    # --- Analytical values for the Toy Problem ---
    model_coeffs = jnp.exp(data_contrib_i + pop_coeff + estimated_b_i)
    
    A = jnp.eye(time_mask_y_i.shape[0], pop_coeff.shape[0])
    S_simple = A @ jnp.diag(model_coeffs)
    H_data = 2 * (S_simple.T @ S_simple) / sigma2[0]
    
    def quadratic_prior_only(b, o2):
        L, _ = jax.scipy.linalg.cho_factor(o2, lower=True)
        return b @ jax.scipy.linalg.cho_solve((L, True), b)
    
    H_prior = jax.hessian(quadratic_prior_only, argnums=0)(estimated_b_i, omega2)
    H_foce = H_data + H_prior

    simple_preds = A @ model_coeffs
    simple_residuals = jnp.where(time_mask_y_i, padded_y_i - simple_preds, 0.0)

    return (estimated_b_i, H_foce, S_simple, simple_residuals, model_coeffs)

def _estimate_b_i_fwd(pop_coeff, sigma2, omega2):
    est_b_i, H_foce, S, res, model_coeffs = _estimate_b_i_impl(pop_coeff, sigma2, omega2)
    return est_b_i, (est_b_i, H_foce, S, res, model_coeffs, pop_coeff, sigma2, omega2)

def _estimate_b_i_bwd(residuals, g_b_i):
    """Backward pass: computes the VJP with the corrected omega gradient."""
    est_b_i, H_foce, S, res, model_coeffs, pop, sig2, om2 = residuals
    
    v = jax.scipy.linalg.solve(H_foce, g_b_i, assume_a='pos')
    
    #Pop Coeff Grad
    S_wrt_log_pc = S
    S_wrt_b = S
    J_cross_pc = (2 / sig2[0]) * (S_wrt_log_pc.T @ S_wrt_b)
    implicit_grad_pc = -v @ J_cross_pc
    explicit_grad_pc = jnp.zeros_like(pop)

    #Sigma Grad
    J_cross_s2 = 2 * (S.T @ res) / sig2[0]**2
    implicit_grad_s2_scalar = -v @ J_cross_s2
    explicit_grad_s2_scalar = 0.0

    #Omega Grad
    def quadratic_prior_only(b, o2):
        """Calculates b.T @ inv(o2) @ b using a stable method."""
        L, _ = jax.scipy.linalg.cho_factor(o2, lower=True)
        return b @ jax.scipy.linalg.cho_solve((L, True), b)

    # --- Implicit Gradient Term ---
    def prior_grad_fn(b, o2):
        # Grad of the quadratic term w.r.t. b
        return jax.grad(quadratic_prior_only, argnums=0)(b, o2)

    # Cross-partial is d/d(o2) of the grad w.r.t. b
    J_cross_o2 = jax.jacobian(prior_grad_fn, argnums=1)(est_b_i, om2)
    implicit_grad_o2 = -v @ J_cross_o2
    
    # --- Explicit Gradient Term ---
    # Grad of the quadratic term w.r.t. o2
    # NOTE THE CORRECTIONS: argnums=1 and the argument order (est_b_i, om2)
    explicit_grad_o2 = jax.grad(quadratic_prior_only, argnums=1)(est_b_i, om2)
    
    grad_omega2 = implicit_grad_o2 + explicit_grad_o2
    
    # --- Return final gradients ---
    return (implicit_grad_pc + explicit_grad_pc, 
            jnp.array([implicit_grad_s2_scalar + explicit_grad_s2_scalar]),
            grad_omega2)

@jax.custom_vjp
def estimate_b_i_final(pop_coeff, sigma2, omega2):
    return _estimate_b_i_impl(pop_coeff, sigma2, omega2)[0]
estimate_b_i_final.defvjp(_estimate_b_i_fwd, _estimate_b_i_bwd)

# ===================================================================
# 4. MRE Test
# ===================================================================
def final_outer_loss(params):
    pop_coeff, sigma2, omega2 = unpack_params(params)
    b_i = estimate_b_i_final(pop_coeff, sigma2, omega2)
    # Use the consistent, stable Cholesky method
    L, _ = jax.scipy.linalg.cho_factor(omega2, lower=True)
    loss = b_i @ jax.scipy.linalg.cho_solve((L, True), b_i)
    return loss

print("--- Running MRE Comparison ---")
fdx_grad_mre = fdx.value_and_fgrad(final_outer_loss)(opt_params)
jax_grad_mre = jax. value_and_grad(final_outer_loss)(opt_params)

print("\nfinitediffx Loss:\n", fdx_grad_mre[0])
print("\njax.grad Loss:\n", jax_grad_mre[0])

print("\nfinitediffx Grad:\n", fdx_grad_mre[1])
print("\njax.grad Grad:\n", jax_grad_mre[1])