import jax
import jax.numpy as jnp
import optax
import finitediffx as fdx
import jaxopt
#from jaxopt.scipy_wrappers import lbfgs_inv_h_dot_v
#from jax.scipy.optimize import lb

#----Introduction----
#This MRE is paired with jaxopt_mre_gemini.py.
#This MRE uses fixed data, optax, a placeholder for the diffeq solve, and a custom vjp implementing the 
#IFT through the inner optax optmizer

#----Purpose----
#The MRE compares the gradient estimated by finite differences over system described in the intro
#to the gradient estimated by AD over the same system where that AD contains a custom vjp defined below. 
#There are several control flow options in the script below, the way those are set can control how the hessian 
#is estimated and/or how the bwd pass quantities are estimated. 

#----Result----
#The loss generated by the two implementation is the same, but the gradients do not match

#----Conclusion----
#Running this example demonstrates that either:
#1) Something related to using optax as the inner optmizer leads to numerical instability
#   in the estimate of the gradient components leading to the custom vjp failing despite the 
#   mathematics in the vjp being correct. 
#   
#   OR
#
#2) The custom vjp mathematics are wrong. 

# ===================================================================
# 1. Hardcoded Inputs
# ===================================================================
# Parameters to be optimized (a flat vector)
opt_params = jnp.array([
    0.47, 1.09, 3.55,  # pop_coeff for ka, cl, vd
    -1.386,            # log(sigma^2) -> so sigma^2 = 0.25
    -0.51, 0.0, -1.20,  # omega cholesky factors
    0.0, 0.0, -2.30
], dtype=jnp.float64)

# Static data for a single subject
padded_y_i = jnp.array([0.74, 2.84, 6.57, 10.5, 9.66, 8.58, 8.36, 7.47, 6.89, 5.94, 3.28], dtype=jnp.float64)
time_mask_y_i = jnp.ones_like(padded_y_i, dtype=bool)
data_contrib_i = jnp.zeros(3, dtype=jnp.float64)
initial_b_i = jnp.zeros(3, dtype=jnp.float64)
pop_coeff_w_bi_idx = jnp.array([0, 1, 2])

# ===================================================================
# 2. Helper Functions (Unpacking, Inner Loss)
# ===================================================================
def unpack_params(params):
    """Unpacks the flat vector into structured parameters."""
    pop_coeff = params[0:3]
    sigma2 = jnp.exp(params[3]) # Note: No squaring, exp(log(sigma^2))
    omega_chol_vals = params[4:10]
    
    omega_lchol = jnp.zeros((3, 3), dtype=jnp.float64).at[jnp.tril_indices(3)].set(omega_chol_vals)
    omega_lchol = omega_lchol.at[jnp.diag_indices(3)].set(jnp.exp(jnp.diag(omega_lchol)))
    omega2 = omega_lchol @ omega_lchol.T
    
    return pop_coeff, jnp.array([sigma2]), omega2 # Ensure sigma2 is shape (1,)

def toy_inner_loss(b_i, pop_coeff, sigma2, omega2):
    """Simplified inner loss using a linear model."""
    model_coeffs_i = jnp.exp(data_contrib_i + pop_coeff + b_i)
    
    A = jnp.eye(time_mask_y_i.shape[0], model_coeffs_i.shape[0])
    pred_y_i = A @ model_coeffs_i
    
    residuals_i = jnp.where(time_mask_y_i, padded_y_i - pred_y_i, 0.0)
    sum_sq_residuals = jnp.sum(residuals_i**2)
    
    loss_data = jnp.sum(time_mask_y_i) * jnp.log(sigma2[0]) + sum_sq_residuals / sigma2[0]
    
    L, _ = jax.scipy.linalg.cho_factor(omega2, lower=True)
    log_det_omega2 = 2 * jnp.sum(jnp.log(jnp.diag(L)))
    #jax.debug.print("log_det_omega2: {o}", o = log_det_omega2)
    prior_penalty = b_i @ jax.scipy.linalg.cho_solve((L, True), b_i)
    #loss_out = loss_data + log_det_omega2 + prior_penalty
    include_logdet_omega = True
    if include_logdet_omega:
        loss_out = loss_data + prior_penalty + log_det_omega2
    else:
        loss_out = loss_data + prior_penalty
    return loss_out

# ===================================================================
# 3. The Inner b_i Estimator using optax and custom vjp
# ===================================================================

def lbfgs_two_loop_recursion(g, s_history, y_history):
    """
    Computes the L-BFGS inverse-Hessian-vector product H_inv @ g.
    This is the core algorithm jaxopt uses internally.
    """
    m = s_history.shape[0]
    q = g
    alpha = jnp.zeros(m)
    
    # First loop (backward)
    def body_fun1(i, inputs):
        q, alpha = inputs
        rho_i = 1.0 / jnp.dot(y_history[i], s_history[i])
        alpha_i = rho_i * jnp.dot(s_history[i], q)
        q = q - alpha_i * y_history[i]
        alpha = alpha.at[i].set(alpha_i)
        return q, alpha
    q, alpha = jax.lax.fori_loop(0, m, lambda i, v: body_fun1(m - 1 - i, v), (q, alpha))

    # Scaling of initial Hessian approximation
    gamma = jnp.dot(y_history[m-1], s_history[m-1]) / jnp.dot(y_history[m-1], y_history[m-1])
    z = q * gamma
    
    # Second loop (forward)
    def body_fun2(i, z):
        rho_i = 1.0 / jnp.dot(y_history[i], s_history[i])
        beta = rho_i * jnp.dot(y_history[i], z)
        z = z + s_history[i] * (alpha[i] - beta)
        return z
    z = jax.lax.fori_loop(0, m, body_fun2, z)
    return z

def _estimate_b_i_impl(pop_coeff, sigma2, omega2):
    """Forward pass: finds b_i and calculates values needed for VJP."""
    
    use_optax = False
    if use_optax:
        optax_obj_fn = lambda b_i: toy_inner_loss(b_i, pop_coeff, sigma2, omega2)
        optimizer = optax.adam(0.05)
        opt_state = optimizer.init(initial_b_i)
        grad_fn = jax.grad(optax_obj_fn)

        def update_step(i, state):
            params, opt_state = state
            grads = grad_fn(params)
            updates, opt_state = optimizer.update(grads, opt_state, params)
            return optax.apply_updates(params, updates), opt_state

        estimated_b_i, _ = jax.lax.fori_loop(0, 10000, update_step, (initial_b_i, opt_state))
        s_history = None
        y_history = None
    else:
        #solver = jaxopt.LBFGS(fun=obj_fn, tol=1e-8, maxiter=5000)
        #solution = solver.run(initial_b_i)
        solver = jaxopt.LBFGS(fun=toy_inner_loss, tol=1e-5, maxiter=500)
        solution = solver.run(initial_b_i, 
                          pop_coeff=pop_coeff, 
                          sigma2=sigma2, 
                          omega2=omega2)
        #print(solution)
        estimated_b_i = solution.params
        s_history = solution.state.s_history
        y_history = solution.state.y_history
    # --- Analytical values for the Toy Problem ---
    model_coeffs = jnp.exp(data_contrib_i + pop_coeff + estimated_b_i)
    # 1. First-order sensitivities (S_simple)
    A = jnp.eye(time_mask_y_i.shape[0], pop_coeff.shape[0])
    S_simple = A @ jnp.diag(model_coeffs)
    
    hessian_calc = 'AD'
    
    if hessian_calc == 'analytical_exact':
        
        # 2. Second-order sensitivities (H_ss)
        # For y = exp(c + b), the second derivative d^2(y)/db^2 is also exp(c + b).
        # H_ss is a tensor representing d^2(y)/db_i/db_j. For our model, it's diagonal.
        H_ss_term = A @ jnp.diag(model_coeffs) 
        H_ss = jnp.einsum('ti,ij->tij', H_ss_term, jnp.eye(pop_coeff.shape[0]))

        # 3. Residuals at the solution
        simple_preds = A @ model_coeffs
        simple_residuals = jnp.where(time_mask_y_i, padded_y_i - simple_preds, 0.0)

        # 4. Assemble the FULL data Hessian
        # H_data = (2/sigma^2) * [S.T @ S - sum(residuals * H_ss)]
        H_data_term1 = S_simple.T @ S_simple
        H_data_term2 = jnp.einsum('tij,t->ij', H_ss, simple_residuals)
        H_data = 2 * (H_data_term1 - H_data_term2) / sigma2[0]

        # 5. Assemble the FULL prior Hessian
        inv_omega2 = jnp.linalg.inv(omega2)
        H_prior = 2 * inv_omega2
        
        # 6. The final, exact Hessian
        H_foce = H_data + H_prior
        # --- END CORRECTION ---
    elif hessian_calc == 'AD':
        #H_foce = jax.hessian(obj_fn)(estimated_b_i)
        H_foce = jax.hessian(toy_inner_loss, argnums=0)(estimated_b_i,  pop_coeff, sigma2, omega2)
    elif hessian_calc == 'GNapprx':
        H_data = 2 * (S_simple.T @ S_simple) / sigma2[0]
        inv_omega2 = jnp.linalg.inv(omega2)
        H_prior = 2 * inv_omega2
        H_foce = H_data + H_prior
        
    simple_preds = A @ model_coeffs
    simple_residuals = jnp.where(time_mask_y_i, padded_y_i - simple_preds, 0.0)

    return (estimated_b_i, H_foce, S_simple, simple_residuals, model_coeffs, s_history, y_history )

def _estimate_b_i_fwd(pop_coeff, sigma2, omega2):
    est_b_i, H_foce, S, res, model_coeffs,s_history, y_history = _estimate_b_i_impl(pop_coeff, sigma2, omega2)
    bwd_resid = (est_b_i, H_foce, S, res, model_coeffs, pop_coeff, sigma2, omega2, s_history, y_history)
    return est_b_i, bwd_resid

def _estimate_b_i_bwd(residuals, g_b_i):
    """Backward pass: computes the VJP with the corrected omega gradient."""
    est_b_i, H_foce, S, res, model_coeffs, pop, sig2, om2, s_history, y_history = residuals
    
    v_method = "other"
    if v_method == "lbfgs":
        v = lbfgs_two_loop_recursion(g_b_i, s_history, y_history)
    else:
        lambda_regularization = 1e-6
        H_foce_regularized = H_foce + lambda_regularization * jnp.eye(H_foce.shape[0])
        v = jax.scipy.linalg.solve(H_foce_regularized, g_b_i, assume_a='pos')
        #v = jax.scipy.linalg.solve(H_foce, g_b_i, assume_a='pos')
    
    grad_b_fn = jax.grad(toy_inner_loss, argnums=0)
    
    #Pop Coeff Grad
    pc_method = "AD"
    if pc_method == 'analytical':
        S_wrt_log_pc = S
        S_wrt_b = S
        J_cross_pc = (2 / sig2[0]) * (S_wrt_log_pc.T @ S_wrt_b)
        implicit_grad_pc = -v @ J_cross_pc
        explicit_grad_pc = jnp.zeros_like(pop)
        grad_pc = implicit_grad_pc + explicit_grad_pc
    elif pc_method == "AD":
        J_cross_pc = jax.jacobian(grad_b_fn, argnums=1)(est_b_i, pop, sig2, om2)
        implicit_grad_pc = -v @ J_cross_pc
        explicit_grad_pc = jnp.zeros_like(pop)
        grad_pc = implicit_grad_pc

    #Sigma Grad
    sigma_method = "AD"
    if sigma_method == 'analytical':
        J_cross_s2 = 2 * (S.T @ res) / sig2[0]**2
        implicit_grad_s2_scalar = -v @ J_cross_s2
        explicit_grad_s2_scalar = 0.0
        grad_s2 = jnp.array([implicit_grad_s2_scalar + explicit_grad_s2_scalar])
    elif sigma_method == "AD":
        J_cross_s2 = jax.jacobian(grad_b_fn, argnums=2)(est_b_i, pop, sig2, om2)
        #implicit_grad_s2_scalar = -v @ J_cross_s2
        #explicit_grad_s2_scalar = 0.0
        grad_s2  = jnp.array([-jnp.dot(v, J_cross_s2.flatten())])

    #Omega Grad
    def quadratic_prior_only(b, o2):
        """Calculates b.T @ inv(o2) @ b using a stable method."""
        L, _ = jax.scipy.linalg.cho_factor(o2, lower=True)
        return b @ jax.scipy.linalg.cho_solve((L, True), b)
    
    omega_method = "AD" #use AD to estimate J_cross_o2
    omega_explicit = False #include explict omega2 grad
    if omega_explicit:
        explicit_grad_o2 = jax.grad(quadratic_prior_only, argnums=1)(est_b_i, om2)
    else:
        explicit_grad_o2 = 0
    if omega_method == 'analytical':
        
        # --- Implicit Gradient Term ---
        def prior_grad_fn(b, o2):
            # Grad of the quadratic term w.r.t. b
            return jax.grad(quadratic_prior_only, argnums=0)(b, o2)

        # Cross-partial is d/d(o2) of the grad w.r.t. b
        J_cross_o2 = jax.jacobian(prior_grad_fn, argnums=1)(est_b_i, om2)
        implicit_grad_o2 = -v @ J_cross_o2
        
        grad_omega2 = implicit_grad_o2 + explicit_grad_o2
    elif omega_method == 'AD':
        
        J_cross_o2 = jax.jacobian(grad_b_fn, argnums=3)(est_b_i, pop, sig2, om2)
        implicit_grad_o2 = -v @ J_cross_o2
        
        grad_omega2 = implicit_grad_o2 + explicit_grad_o2
        
    # --- Return final gradients ---
    return (grad_pc, 
            grad_s2,
            grad_omega2)

@jax.custom_vjp
def estimate_b_i_final(pop_coeff, sigma2, omega2):
    return _estimate_b_i_impl(pop_coeff, sigma2, omega2)[0]
estimate_b_i_final.defvjp(_estimate_b_i_fwd, _estimate_b_i_bwd)

# ===================================================================
# 4. MRE Test
# ===================================================================
def final_outer_loss(params):
    pop_coeff, sigma2, omega2 = unpack_params(params)
    b_i = estimate_b_i_final(pop_coeff, sigma2, omega2)
    # Use the consistent, stable Cholesky method
    L, _ = jax.scipy.linalg.cho_factor(omega2, lower=True)
    loss = b_i @ jax.scipy.linalg.cho_solve((L, True), b_i)
    return loss

print("--- Running MRE Comparison ---")
fdx_grad_mre = fdx.value_and_fgrad(final_outer_loss)(opt_params)
jax_grad_mre = jax.value_and_grad(final_outer_loss)(opt_params)

print("\nfinitediffx Loss:\n", fdx_grad_mre[0])
print("\njax.grad Loss:\n", jax_grad_mre[0])

print("\nfinitediffx Grad:\n", fdx_grad_mre[1])
print("\njax.grad Grad:\n", jax_grad_mre[1])